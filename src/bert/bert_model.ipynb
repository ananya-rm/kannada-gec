{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cf8a47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logger' from 'util' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'logger' from 'util' (unknown location)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers as ppb\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "from util import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de17d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_model():\n",
    "    def __init__(self):\n",
    "        self.is_loaded = False\n",
    "\n",
    "    def load_BERT(self, small=True):\n",
    "        \"\"\"Loads the pretrained BERT models and the corresponding Tokenizers\n",
    "\n",
    "        Keyword Arguments:\n",
    "            small {bool} -- Whether to load the smaller model or the bigger one (default: {True})\n",
    "\n",
    "        Returns:\n",
    "            [tuple] -- Pretrained DistilBERT or BERT and the corresponding Tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_loaded:\n",
    "            logging.warning('BERT is already loaded!')\n",
    "\n",
    "        if small:\n",
    "            # For DistilBERT:\n",
    "            model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-multilingual-cased')\n",
    "        else:\n",
    "            # For BERT\n",
    "            model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-multilingual-cased')\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        self.model = model_class.from_pretrained(pretrained_weights)\n",
    "        self.is_loaded = True\n",
    "        logging.info('BERT has been loaded successfully')\n",
    "\n",
    "    def tokenize_sentence(self, sent):\n",
    "        \"\"\"Tokenizes the given sentence sent\n",
    "\n",
    "        Arguments:\n",
    "            sent {str} -- Sentence to be tokenized\n",
    "\n",
    "        Returns:\n",
    "            list -- List of tokens\n",
    "        \"\"\"\n",
    "        # TODO take batch of sentences\n",
    "        return self.tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    def get_padded_and_attention_mask(self, tokenized_df, max_len):\n",
    "        \"\"\"Pad the tokenized sentences and Generate the correspoding mask array\n",
    "\n",
    "        Arguments:\n",
    "            tokenized_df {pandas.DataFrame} -- Dataframe of shape (*, 1) containing tokenized sentences\n",
    "            max_len {int} -- Maximum num of tokens among all the sentences\n",
    "\n",
    "        Returns:\n",
    "            tuple(ndarray,ndarray) -- Tuple containing (padded ndarray, mask array)\n",
    "        \"\"\"\n",
    "        padded_ndarray = np.array([i + [0]*(max_len-len(i)) for i in tokenized_df.values])\n",
    "        logging.info('Padded array shape:{}'.format(padded_ndarray.shape))\n",
    "\n",
    "        # Create the attention mask\n",
    "        attention_mask_ndarray = np.where(padded_ndarray != 0, 1, 0)\n",
    "        logging.info('Attention mask shape:{}'.format(attention_mask_ndarray.shape))\n",
    "\n",
    "        return (padded_ndarray, attention_mask_ndarray)\n",
    "\n",
    "    def get_bert_embeddings(self, padded_feature_array, attention_mask_array, batch_size=-1):\n",
    "        \"\"\"Runs the BERT model on the given padded_feature_array and attention_mask_array to get the embeddings.\n",
    "\n",
    "        Arguments:\n",
    "            padded_feature_array {numpy.ndarray} -- Tokenized and padded sentence features\n",
    "            attention_mask_array {numpy.ndarray} -- The mask indicating the position of sentence features\n",
    "\n",
    "        Keyword Arguments:\n",
    "            batch_size {int} -- To avoid memory errors, process the sentences in batches (default: {-1})\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor -- Returns torch.tensor containing BERT output weights as embeddings\n",
    "        \"\"\"\n",
    "        logging.info('Going to get BERT embeddings for {} records'.format(padded_feature_array.shape[0]))\n",
    "        if batch_size > 0:\n",
    "            # Generate embeddings batch-by-batch\n",
    "            logging.info('Running batch-wise. Original shape:{}'.format(padded_feature_array.shape))\n",
    "            # Limit the max batch size\n",
    "            max_size = padded_feature_array.shape[0]\n",
    "            batch_size = max_size if batch_size > padded_feature_array.shape[0] else batch_size\n",
    "            final_embeddings = None\n",
    "            logging.debug('-' * 30)\n",
    "            for b_start in range(0, max_size, batch_size):\n",
    "                b_end = b_start + batch_size\n",
    "                b_num = int(b_start/batch_size)\n",
    "                logging.debug('Batch {} start:{}, end:{}'.format(b_num, b_start, b_end))\n",
    "                batch_sents = padded_feature_array[b_start:b_end]\n",
    "                batch_attentions = attention_mask_array[b_start:(b_start + batch_size)]\n",
    "                # Get the embeddings for this batch\n",
    "                batch_embeddings_tensor = self.__get_embeddings_per_batch(batch_sents, batch_attentions)\n",
    "                logging.debug('Batch emb size:{}'.format(batch_embeddings_tensor.size()))\n",
    "                # Accumulate the embeddings generated so far\n",
    "                if final_embeddings == None:\n",
    "                    final_embeddings = batch_embeddings_tensor\n",
    "                else:\n",
    "                    final_embeddings = torch.cat([final_embeddings, batch_embeddings_tensor], dim=0)\n",
    "                    logging.info('Accumulated emb size:{}'.format(final_embeddings.size()))\n",
    "                # End of for\n",
    "                logging.debug('-' * 30)\n",
    "            # Return the final embeddings\n",
    "            return final_embeddings\n",
    "        else:\n",
    "            return self.__get_embeddings_per_batch(padded_feature_array, attention_mask_array)\n",
    "\n",
    "    def __get_embeddings_per_batch(self, padded_feature_array, attention_mask_array):\n",
    "        input_token_id_matrix = torch.tensor(padded_feature_array)\n",
    "        attention_mask = torch.tensor(attention_mask_array)\n",
    "        # No training of BERT now!\n",
    "        with torch.no_grad():\n",
    "            t0 = time.time()\n",
    "            output_hidden_states = self.model(input_token_id_matrix, attention_mask=attention_mask)\n",
    "            output_hidden_states = output_hidden_states[0]\n",
    "            t1 = time.time()\n",
    "            logging.info('Time taken:{} seconds'.format(int(t1-t0)))\n",
    "            return output_hidden_states\n",
    "\n",
    "    def convert_tokenized_sent_to_bert_emb(self, tokenized_df, MAX_LEN, batch_size=-1):\n",
    "        \"\"\"Converts the given tokenized dataframe of sentence tokens into bert embeddings\n",
    "\n",
    "        Arguments:\n",
    "            tokenized_df {pandas.DataFrame} -- Dataframe of shape (*, 1) containing tokenized sentences\n",
    "            max_len {int} -- Maximum num of tokens among all the sentences\n",
    "\n",
    "        Keyword Arguments:\n",
    "            batch_size {int} -- To avoid memory errors, process the sentences in batches (default: {-1})\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor -- Returns torch.tensor containing BERT output weights as embeddings\n",
    "        \"\"\"\n",
    "        # Pad and get the attention mask arrays\n",
    "        padded_ndarray, attention_mask_ndarray = self.get_padded_and_attention_mask(tokenized_df, MAX_LEN)\n",
    "        # Feed the padded array and attention mask array to the bert model and\n",
    "        #  get the hidden states from output layer of BERT\n",
    "        return self.get_bert_embeddings(padded_ndarray, attention_mask_ndarray, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039c24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
